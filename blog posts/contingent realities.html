<!DOCTYPE html>
<html lang=“en”>
  <head>
    <meta charset=“UTF-8”>
    <meta name=“viewport” content=“width=device-width, initial-scale=1”>
    <title>Lewis Lloyd</title>
    <link rel="stylesheet" type="text/css" href="site-style.css">
  </head>

  <body>
    <h1><br><br>
      <a style="color:snow", href="../index.html">Lewis&nbsp; &nbsp; &nbsp; &nbsp;*</a><br>
      <a style="color:snow", href="../index.html">*&nbsp; &nbsp; &nbsp; &nbsp;Lloyd</a><br>
    </h1>


    <!--<h2><br>
      December
    </h2>

    <p>
      <em>Gogol's Wife</em> - Tommaso Landolfi
    </p>-->

    <p style="text-align: right; margin-left:130px; margin-right:150px">
      back to <a href="../blog.html">blog</a>
    </p>

    <h2 style="margin-left:130px; margin-right:150px"><br>
      Contingent Realities
    </h2>

    <p style="font-size:14pt; margin-left:130px; margin-right:150px; font-weight: 100;">
      In late September, this excellent Sara Hooker <a href="https://arxiv.org/pdf/2009.06489.pdf">article</a> was doing the rounds.
      I read it in a London park, back when the weather was still warm and COVID in retreat.<br><br>

      Hooker makes the case, persuasively, that the success of different approaches to AI has been determined more by their affinity
      with the available computer hardware (and software, although she focuses on it less) than by the quality of the ideas themselves.
      Neural networks are a classic example: they were disregarded in AI research for decades partly because they they didn't work very
      well on the general-purpose CPUs that were available to researchers in the 1960s, '70s and '80s. It was only in the 2000s, when
      by 'fluke' researchers started repurposing graphical processing units (GPUs) - chips originally designed for working with
      computer graphics and very popular with gamers - for training deep neural nets, that their potential became apparent.
      (If you're interested in <em>why</em> GPUs worked so well for this, see the article itself.)
      Neural networks now underpin the world's most sophisticated AI systems.<br><br>

      The story doesn't end there, though. Hooker warns that our current obsession with neural nets - the erstwhile losers now firmly
      established as winners of the 'hardware lottery' - risks crowding out other approaches to AI that might be crucial for further
      advances. Increasing emphasis is being placed on, and money invested in, developing specialised hardware for deep neural networks,
      to overcome the diminishing returns from more generalisable hardware (as Moore's law slows down etc). But this increasing
      specialisation 'makes it even more costly to stray off the beaten track of research ideas', increasing the risk of missing other
      potentially transformative approaches.<br><br>
    </p>

    <p>

    </p>

  </body>
